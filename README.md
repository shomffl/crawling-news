# CRAWLING_NEWS

# 環境構築

```bash

pip install feedparser
pip install -U firebase-admin
pip install google-cloud-firestore
pip install python-dotenv

```
- firebaseでプロジェクトを作成して、認証ファイルを生成
- 生成した認証ファイルをルートディレクトリに置く
- ルートディレクトリに.envファイルを作成
- .envファイルに「FIREBASE_API_KEY=ダウンロードした認証ファイル名」 を追加

# APIについて
- crawling.pyについて  
ニュースの情報をクローリングするためのAPI。引数としてRSSのURLを渡す必要がある。二次元リストを返して、二次元リストを構成する一次元リストはニュース記事ごとに「タイトル、要約、リンク」をまとめたもの。

- convert_news_data.pyについて  
クローリングしたニュースデータをjson形式にしてフォルダーに格納するためのAPI。引数として二次元リストと保存先のファイルパスを指定する必要がある。二次元リストに関してはcrawling.pyで作成したものをそのまま渡せば大丈夫。保存先のファイルパスに関しては、ファイルは自動で生成されるため作成する必要はないが、フォルダーは作成されないため、事前に作成する必要がある。

- get_book_data1.pyについて  
openBDのAPIを使用して本の情報を得るためのAPI。APIの使用に登録などは必要ない。引数としてISBNコードを渡す必要がある。入手したデータの中に本の要約が含まれていたら、要約を文字列として返し、要約を入手できなかった場合は、空の変数を返す。

- get_book_data2.pyについて  
楽天ブックスを使用して本の情報を得るためのAPI。APIを使うには登録する必要がある（楽天IDが必要）。引数としてISBNコードと楽天ブックスのAPI_IDを渡す必要がある。また、1秒に1リクエストまでの制限がある。入手したデータの中に本の要約が含まれていたら、要約を文字列として返し、要約を入手できなかった場合は、空の変数を返す。

- get_summary.pyについて  
get_book_data1.pyとget_book_data2.pyを動かすためのAPI。引数としてISBNコードを渡す必要がある。最初にget_book_data1.pyを実行して本の要約を入手できた場合は、返ってきた文字列を変数に格納して処理を終了する。仮に、get_book_data1.pyで本の要約を入手できなかった場合は、get_book_data2.pyで取得を試みる。  
openBDと楽天ブックスで同じ本が登録されていた場合、中身のデータは同じであるため、openBDでデータを取得できた場合は、楽天ブックスのAPIは叩く必要はないと判断した。


- convert_book_data.pyについて  
本の情報をjson形式にしてフォルダーに格納するためのAPI。引数として複数の本の情報を「本のタイトル、ISBNコード」の順で、二次元リストにしたものと、保存先のファイルパスを渡す必要がある。
